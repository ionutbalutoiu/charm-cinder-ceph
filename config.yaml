options:
  ceph-osd-replication-count:
    default: 3
    type: int
    description: |
      This value dictates the number of replicas ceph must make of any
      object it stores within the cinder rbd pool. Of course, this only
      applies if using Ceph as a backend store. Note that once the cinder
      rbd pool has been created, changing this value will not have any
      effect (although it can be changed in ceph by manually configuring
      your ceph cluster).
  ceph-pool-weight:
    type: int
    default: 40
    description: |
      Defines a relative weighting of the pool as a percentage of the total
      amount of data in the Ceph cluster. This effectively weights the number
      of placement groups for the pool created to be appropriately portioned
      to the amount of data expected. For example, if the ephemeral volumes
      for the OpenStack compute instances are expected to take up 20% of the
      overall configuration then this value would be specified as 20. Note -
      it is important to choose an appropriate value for the pool weight as
      this directly affects the number of placement groups which will be
      created for the pool. The number of placement groups for a pool can
      only be increased, never decreased - so it is important to identify the
      percent of data that will likely reside in the pool.
  backend-availability-zone:
    default:
    type: string
    description: |
      Availability zone name of this volume backend. If set, it will
      override the default availability zone. Supported for Pike or
      newer releases.
  use-syslog:
    type: boolean
    default: False
    description: |
      Setting this to True will configure services to log to syslog.
  restrict-ceph-pools:
    default: False
    type: boolean
    description: |
      Optionally restrict Ceph key permissions to access pools as required.
  rbd-pool-name:
    default:
    type: string
    description: |
      Optionally specify an existing rbd pool that cinder should map to.
  rbd-flatten-volume-from-snapshot:
    default:
    type: boolean
    default: False
    description: |
      Flatten volumes created from snapshots to remove dependency from
      volume to snapshot. Supported on Queens+
  pool-type:
    type: string
    default: replicated
    description: |
      Ceph pool type to use for storage - valid values include ‘replicated’
      and ‘erasure-coded’.
  ec-profile-name:
    type: string
    default:
    description: |
      Name for the EC profile to be created for the EC pools. If not defined
      a profile name will be generated based on the name of the pool used by
      the application.
  ec-rbd-metadata-pool:
    type: string
    default:
    description: |
      Name of the metadata pool to be created (for RBD use-cases). If not
      defined a metadata pool name will be generated based on the name of
      the data pool used by the application.  The metadata pool is always
      replicated, not erasure coded.
  ec-profile-k:
    type: int
    default: 1
    description: |
      Number of data chunks that will be used for EC data pool. K+M factors
      should never be greater than the number of available zones (or hosts)
      for balancing.
  ec-profile-m:
    type: int
    default: 2
    description: |
      Number of coding chunks that will be used for EC data pool. K+M factors
      should never be greater than the number of available zones (or hosts)
      for balancing.
  ec-profile-locality:
    type: int
    default:
    description: |
      (lrc plugin - l) Group the coding and data chunks into sets of size l.
      For instance, for k=4 and m=2, when l=3 two groups of three are created.
      Each set can be recovered without reading chunks from another set. Note
      that using the lrc plugin does incur more raw storage usage than isa or
      jerasure in order to reduce the cost of recovery operations.
  ec-profile-crush-locality:
    type: string
    default:
    description: |
      (lrc plugin) The type of the crush bucket in which each set of chunks
      defined by l will be stored. For instance, if it is set to rack, each
      group of l chunks will be placed in a different rack. It is used to
      create a CRUSH rule step such as step choose rack. If it is not set,
      no such grouping is done.
  ec-profile-durability-estimator:
    type: int
    default:
    description: |
      (shec plugin - c) The number of parity chunks each of which includes
      each data chunk in its calculation range. The number is used as a
      durability estimator. For instance, if c=2, 2 OSDs can be down
      without losing data.
  ec-profile-helper-chunks:
    type: int
    default:
    description: |
      (clay plugin - d) Number of OSDs requested to send data during
      recovery of a single chunk. d needs to be chosen such that
      k+1 <= d <= k+m-1. Larger the d, the better the savings.
  ec-profile-scalar-mds:
    type: string
    default:
    description: |
      (clay plugin) specifies the plugin that is used as a building
      block in the layered construction. It can be one of jerasure,
      isa, shec (defaults to jerasure).
  ec-profile-plugin:
    type: string
    default: jerasure
    description: |
      EC plugin to use for this applications pool. The following list of
      plugins acceptable - jerasure, lrc, isa, shec, clay.
  ec-profile-technique:
    type: string
    default:
    description: |
      EC profile technique used for this applications pool - will be
      validated based on the plugin configured via ec-profile-plugin.
      Supported techniques are ‘reed_sol_van’, ‘reed_sol_r6_op’,
      ‘cauchy_orig’, ‘cauchy_good’, ‘liber8tion’ for jerasure,
      ‘reed_sol_van’, ‘cauchy’ for isa and ‘single’, ‘multiple’
      for shec.
  ec-profile-device-class:
    type: string
    default:
    description: |
      Device class from CRUSH map to use for placement groups for
      erasure profile - valid values: ssd, hdd or nvme (or leave
      unset to not use a device class).
